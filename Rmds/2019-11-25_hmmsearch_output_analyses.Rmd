---
title: "HMM search output analyses with avg e-val:s"
author: "LJM"
date: "`r Sys.Date()`"
link-citations: true
bibliography: bibliography.bib
biblio-style: "alpha" #https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles
#csl: "../uppsala-universitet-institutionen-for-biologisk-grundutbildning.csl"
documentclass: report
output:
  bookdown::html_document2:
    highlight: tango
    theme: journal
    split_by: none # only generate a single output page
    self_contained: TRUE
    toc: yes
    toc_float: 
      collapsed: TRUE
      smooth_scroll: TRUE
      print: FALSE
    code_folding: show
    number_sections: TRUE
    code_download: TRUE
  html_document: 
    toc: TRUE
    toc_float: TRUE
    code_folding: show
    highlight: tango
    number_sections: TRUE
    self_contained: TRUE
    smart: TRUE # To be figured out when this is useful
    theme: journal
  pdf_document:
    toc: TRUE
    toc_depth: 2
    highlight: tango # This appears to be the same as default
    number_sections: TRUE
  bookdown::html_document2:
    highlight: tango
    theme: journal
    split_by: none # only generate a single output page
    self_contained: TRUE
    toc: yes
    toc_float: 
      collapsed: TRUE
      smooth_scroll: TRUE
      print: FALSE
    code_folding: show
    number_sections: TRUE
    code_download: TRUE
---

# Load libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(vegan)
library(ggbiplot)
```

# Read intermediary `csv` file

In the intermediary `csv` file the $-log_{10}(\text{e-val})$:s are the average e-values of all matches with same HMM profiles to each fOTU.

```{r eval=FALSE, include=TRUE}
columns <- paste("c", strrep("d",8315), sep = "")
fOTUsHMM <- read_csv(file = "../analyses/HMMsearch/avg_fOTUsHMM.csv", col_types = columns)
```

# Exchange NA, Inf and -Inf with 0

```{r}
is.na(fOTUsHMM) <- sapply(fOTUsHMM, is.infinite)
fOTUsHMM[is.na(fOTUsHMM)] <- 0
```

# Move fOTU names to row names and execute PCA 

```{r}
fOTUsHMM_pca <- fOTUsHMM %>% 
  column_to_rownames(var = "fOTU_name") %>%
  prcomp(., 
         center = TRUE)
```

# Visualise the results

The following graph depicts how large percentage of bins in fOTU have at least one hit from viral HMM profiles with e-value less than 0.01.

```{r}
file_name <- "../visualisations/HMMsearch/ggbiplot_avg_PC1-2.png"
png(filename = file_name)
ggbiplot(fOTUsHMM_pca, var.axes = F, choices = 1:2)
dev.off()
ggbiplot(fOTUsHMM_pca, var.axes = F, choices = 1:2)
```


```{r}
prediction <- predict(fOTUsHMM_pca)[,1:20]
file_name <- "../visualisations/HMMsearch/heatmap_avg_20_first_PCs.png"
png(filename = file_name)
heatmap(as.matrix(prediction))
dev.off()
heatmap(as.matrix(prediction))

file_name <- "../visualisations/HMMsearch/heatmap_avg_400_first_fOTUs.png"
png(filename = file_name)
heatmap(as.matrix(fOTUsHMM[,-1])[,1:400])
dev.off()
heatmap(as.matrix(fOTUsHMM[,-1])[,1:400])
```

# NMDS

Let's perform non-metric multidimensional scaling:

```{r}
width <- dim(fOTUsHMM)[2]
ord_avg <- metaMDS(fOTUsHMM[,2:width])
```

# Save/Load R data

Save or load the data once again to speed-up things.

```{r eval=FALSE, include=TRUE}
#saveRDS(ord_avg, file = "../analyses/HMMsearch/R_data/avg_ord.rds")
ord_avg <- readRDS(file = "../analyses/HMMsearch/R_data/avg_ord.rds")
```

How well does the 2-D representation of multidimensional space represent the actual multidimensional space is done by using statistic called stress. Stress < 0.1 indicates that the 2-D representation is a good repreresentation of the multi-D space. Let's check that out now:

```{r}
ord_avg$stress
```

This is quite low which is great! Let's now plot out the relationship between the ordination dissimilarities and the distances in the ordination space:

```{r}
stressplot(ord_avg)
```

Observed dissimilarity of 1 means the fOTU doesn't have any HMMs incommon with the others.

Let's plot out the ordination space in 2-d:

```{r}
ordiplot(ord_avg)
points(ord_avg)
```

and let's filter away the outliers and make new plot from above:

```{r}
NMDS_points1 <- as_tibble(ord_avg$points) %>%
  filter(MDS1 > -10)

title <- "NMDS, MDS1 > -10"
qplot(MDS1, 
      MDS2, 
      data = NMDS_points1, 
      geom = c("point", "density2d")) + ggtitle(title)

path <- "../visualisations/HMMsearch/nmds_hmmsearch_avg_MDS1_greater_than_-10.png"

ggsave(filename = path,
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)
```

Let's filter away slightly more:

```{r}
NMDS_points2 <- as_tibble(ord_avg$points) %>%
  filter(MDS1 > -2)

title <- "NMDS, -2 < MDS1"
qplot(MDS1, 
      MDS2, 
      data = NMDS_points2, 
      geom = c("point", "density2d")) + ggtitle(title)

path <- "../visualisations/HMMsearch/nmds_hmmsearch_avg_MDS1_greater_than_-2.png"

ggsave(filename = path,
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)
```



# Create lists of fOTUs of interest

## Preview PCA plots

These PCA plots can be used to visually determine potential groupings.

```{r}

PC12s <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC1 = unname(fOTUsHMM_pca$x[,1]), 
              PC2 = unname(fOTUsHMM_pca$x[,2]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC1, 
      PC2, 
      data = PC12s, 
      geom = c("point", "density2d")) + ggtitle(title)

ggsave(filename = "../visualisations/HMMsearch/pca_hmmsearch_avg_PC1-2.png",
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)

PC23s <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC2 = unname(fOTUsHMM_pca$x[,2]), 
              PC3 = unname(fOTUsHMM_pca$x[,3]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC2, 
      PC3, 
      data = PC23s, 
      geom = c("point", "density2d")) + ggtitle(title)

ggsave(filename = "../visualisations/HMMsearch/pca_hmmsearch_avg_PC2-3.png",
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)

PC34s <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC3 = unname(fOTUsHMM_pca$x[,3]), 
              PC4 = unname(fOTUsHMM_pca$x[,4]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC3, 
      PC4, 
      data = PC34s, 
      geom = c("point", "density2d")) + ggtitle(title)

ggsave(filename = "../visualisations/HMMsearch/pca_hmmsearch_avg_PC3-4.png",
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)
```

## Create a file for each list

### Create file names

```{r}
# Path to files
path <- "../analyses/lists_of_PCA_clusters/"
# Names of files
PC12a_filename <- paste(path,
                        "fOTU_avg_PC1-and-2_x--100-to-0-and-y--100-to-+100.txt", 
                        sep = "", 
                        collapse = NULL)
PC12b_filename <- paste(path,
                        "fOTU_avg_PC1-and-2_x-+250-to-+400-and-y--100-to-0.txt", 
                        sep = "", 
                        collapse = NULL)
PC23_filename <- paste(path,
                       "fOTU_avg_PC2-and-3_x--100-to-+50-and-y--100-to-+100.txt", 
                       sep = "", 
                       collapse = NULL)
PC34_filename <- paste(path,
                       "fOTU_avg_PC3-and-4_x--50-to-+50-and-y--50-to-+75.txt", 
                       sep = "", 
                       collapse = NULL)
```

### Filter tibbles based on coordinates

```{r}
# This function filters a tibble based on given limit values
filterator <- function(data, x_low, x_high, y_low, y_high){
  data %>% 
    filter(data[,2] > x_low & data[,2] < x_high & data[,3] > y_low & data[,3] < y_high) %>%
    select(fOTUs)
}
```


```{r}
PC12_cluster1 <- filterator(PC12s,-100,0,-100,100) 
PC12_cluster2 <- filterator(PC12s,250,400,-100,0) 
PC23_cluster <- filterator(PC23s,-100,50,-100,100)
PC34_cluster <- filterator(PC34s,-50,50,-50,75)
```

### Create the files

```{r}
write_delim(PC12_cluster1, 
            PC12a_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
write_delim(PC12_cluster2, 
            PC12b_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
write_delim(PC23_cluster, 
            PC23_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
write_delim(PC34_cluster, 
            PC34_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
```


# Not used at the moment

This following is not run because they are not of interest at the moment.

```{r eval=FALSE, include=TRUE}
fOTUbinNums <- read_csv(file = "../analyses/numBinsfOTU.csv", 
                          col_types = "ci", 
                          col_names = c("fOTU_name",
                                        "num_bins"))
egg_nog_cats <- 
  read_tsv(file = "../data/annotations/10239_annotations.tsv", 
           col_types = "fcfc", 
           col_names = c("taxid","hmm_profile_id",
                         "egg_nog_category",
                         "hmm_description")) %>%
  # Drop taxid because it's uninteresting
  select(.,-taxid)

num_seqs <- read_csv(file = "../analyses/numSeqsBin.csv", 
                          col_types = "ci")
```


```{r eval=FALSE, include=TRUE}
# Make a big tibble out of these three tibbles by left joining 
fOTU <- left_join(fOTU, 
                  egg_nog_cats, 
                  by = "hmm_profile_id") %>%
  left_join(., 
            fOTUbinNums, 
            by = "fOTU_name") %>%
  # Remove values that were outside inclusion threshold
  filter(.,inside_inclusion_threshold) %>%
  # Drop inside_inclusion_threshold now that it has done its duty
  select(.,-inside_inclusion_threshold)

  # Find how many hits there are to each bin
  numHits <- fOTU %>%
    count(.,Target_Bin_id) %>%
    rename(.,Num_hits = n)
  
  # Add num hits beside each bin
  fOTU <- left_join(fOTU, numHits, by = "Target_Bin_id")
  
  # Add total number of sequences in each bin
  fOTU <- left_join(fOTU, num_seqs, by = "Target_Bin_id")
  
  # Count how many unique hits to bins there are
  fOTU_vec_len <- fOTU %>%
    distinct(.,Target_Bin_id) %>%
    pull(Target_Bin_id) %>%
    length()
  
  # Grab the total number of bins in the fOTU
  fOTU_tot_num_bins <- fOTU %>%
    pull(num_bins)
  fOTU_tot_num_bins <- fOTU_vec_len/fOTU_tot_num_bins[1]
  
  # Append the value to a vector
#  hit_percentage <- c(hit_percentage,fOTU_tot_num_bins)

#}

# Create finally a tibble from the vector
#hits <- tibble(hit_percentage)
```


# Session info

```{r}
sessionInfo()
```


# References
