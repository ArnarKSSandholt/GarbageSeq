---
title: "HMM search output analyses with avg e-val:s"
author: "LJM"
date: "`r Sys.Date()`"
link-citations: true
bibliography: bibliography.bib
biblio-style: "alpha" #https://www.overleaf.com/learn/latex/Bibtex_bibliography_styles
#csl: "../uppsala-universitet-institutionen-for-biologisk-grundutbildning.csl"
documentclass: report
output:
  bookdown::html_document2:
    highlight: tango
    theme: journal
    split_by: none # only generate a single output page
    self_contained: TRUE
    toc: yes
    toc_float: 
      collapsed: TRUE
      smooth_scroll: TRUE
      print: FALSE
    code_folding: show
    number_sections: TRUE
    code_download: TRUE
  html_document: 
    toc: TRUE
    toc_float: TRUE
    code_folding: show
    highlight: tango
    number_sections: TRUE
    self_contained: TRUE
    smart: TRUE # To be figured out when this is useful
    theme: journal
  pdf_document:
    toc: TRUE
    toc_depth: 2
    highlight: tango # This appears to be the same as default
    number_sections: TRUE
  bookdown::html_document2:
    highlight: tango
    theme: journal
    split_by: none # only generate a single output page
    self_contained: TRUE
    toc: yes
    toc_float: 
      collapsed: TRUE
      smooth_scroll: TRUE
      print: FALSE
    code_folding: show
    number_sections: TRUE
    code_download: TRUE
---

# Load libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(vegan)
library(ggbiplot)
```

# Initilise variables

```{r, message=FALSE, cache=TRUE}
#fOTUsHMM <- read_csv(file = "../analyses/fOTU_HMM_headers.csv")
fOTUsHMM <- tibble()

path_to_data <- 
  "../analyses/HMM_scan_using_eggNOG_HMMs/hmmsearch_out_parsed/"

files <- list.files(path = path_to_data, 
                    pattern = "*.tsv", recursive=FALSE)

# Read in some data
tsv_names <- c("fOTU_name",
               "hmm_profile_id",
               "inside_inclusion_threshold",
               "Target_Bin_id",
               "Target_Seq_id",
               "full_sequence_e_value",
               "full_sequence_score",
               "full_sequence_bias",
               "best_one_domain_e-value",
               "best_one_domain_score",
               "best_one_domain_bias",
               "exp",
               "N",
               "description")
```

# Read data and create a tibble out of them

```{r, cache=TRUE}
# Go through each tsv parsed results file
for(fOTU_file in files) {
  # Create the file path
  path_file <- paste(path_to_data,fOTU_file, sep = "")
  
  # Create a tibble from a tsv file
  fOTU <- read_tsv(file = path_file, 
                   col_types = "cclcfdddddddic", 
                   col_names = tsv_names) %>%
    # Take only interesting columns
    select(.,c("fOTU_name",
               "hmm_profile_id",
               "full_sequence_e_value")) %>%
    # Create a new col with -log_10 e-values
    mutate(log_eval = -1*log10(full_sequence_e_value)) %>%
    # Drop the old e-value column
    select(.,-full_sequence_e_value) 
  
  # Check if there were empty bins
  fOTU_first_term <- fOTU %>%
    pull(fOTU_name) # This returns a vector of fOTU names
  # Pick just first element because that 
  # would be where my "dummy" text would reside
  fOTU_first_term <- fOTU_first_term[1]
  
  # Check if the results had no hits
  if(fOTU_first_term == "dummy"){
    # For now just jump over those fOTUs with zero hits
    #fOTUsHMM <- bind_rows(fOTUsHMM,tibble())
    next
  }else{
    # If there were matches spread the -log_10 evalues for each
    # fOTU in one row
    fOTU <- fOTU %>%
      # Group all same HMMs together
      group_by(hmm_profile_id) %>%
      # Choose average of all e-values for each unique HMM
      summarise(avg_log_eval = mean(log_eval)) %>%
      add_column(fOTU_name = fOTU_first_term) %>%
      # Finally, spread the e-vals on one row
      spread(hmm_profile_id,avg_log_eval)
  }
  # Append the current fOTU data to one big tibble
  fOTUsHMM <- bind_rows(fOTUsHMM,fOTU)
}
```

# Save R data

The previous chunk with 1432 fOTUs took about 2 h 14 minutes to run with one Intel Core i7-8550U CPU @ 1.80GHz (turbo 1.99GHz). So in order to save up time from previous calculations again, let's store the R data:

```{r eval=FALSE, include=TRUE}
saveRDS(fOTUsHMM, file = "../analyses/temp_R_data/fOTUsHMM.rds")
save(fOTUsHMM, file = "../analyses/temp_R_data/fOTUsHMM.RData")
```

# Load R data

```{r eval=FALSE, include=TRUE}
#fOTUsHMM <-  load(file = "../analyses/temp_R_data/fOTUsHMM.RData")
fOTUsHMM <- readRDS(file = "../analyses/temp_R_data/fOTUsHMM.rds")
```


# Exchange NA, Inf and -Inf with 0

```{r}
is.na(fOTUsHMM) <- sapply(fOTUsHMM, is.infinite)
fOTUsHMM[is.na(fOTUsHMM)] <- 0
```

# Move fOTU names to row names and execute PCA 

```{r}
fOTUsHMM_pca <- fOTUsHMM %>% 
  column_to_rownames(var = "fOTU_name") %>%
  prcomp(., 
         center = TRUE)
```


# Visualise the results

The following graph depicts how large percentage of bins in fOTU have at least one hit from viral HMM profiles with e-value less than 0.01.

```{r}
PCs <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC1 = unname(fOTUsHMM_pca$x[,1]), 
              PC2 = unname(fOTUsHMM_pca$x[,2]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC1, 
      PC2, 
      data = PCs, 
      geom = c("point", "density2d")) + ggtitle(title)
```


```{r message=FALSE, warning=FALSE}
library(ggbiplot)
```

```{r}
ggbiplot(fOTUsHMM_pca, var.axes = F, choices = 4:3)
```


```{r}
heatmap(as.matrix(predict(fOTUsHMM_pca)[,1:20]))
dim(fOTUsHMM_pca)
heatmap(as.matrix(fOTUsHMM[,-1])[,1:400])

```



# NMDS

Let's perform non-metric multidimensional scaling:

```{r}
#install.packages("vegan")
library(vegan)
```


```{r}
ord1 <- metaMDS(fOTUsHMM[,2:8290])
```

# Save R data

Save the data once again to speed-up things.

```{r eval=FALSE, include=TRUE}
saveRDS(ord1, file = "../analyses/HMMsearch/temp_R_data/ord1.rds")
```

# Load R data

```{r eval=FALSE, include=TRUE}
ord1 <- readRDS(file = "../analyses/HMMsearch/temp_R_data/ord1.rds")
```


How well does the 2-D representation of multidimensional space represent the actual multidimensional space is done by using statistic called stress. Stress < 0.1 indicates that the 2-D representation is a good repreresentation of the multi-D space. Let's check that out now:

```{r}
ord1$stress
```

This is quite low which is great! Let's now plot out the relationship between the ordination dissimilarities and the distances in the ordination space:

```{r}
stressplot(ord1)
```

Observed dissimilarity of 1 means the fOTU doesn't have any HMMs incommon with the others.

Let's plot out the ordination space in 2-d:

```{r}
ordiplot(ord1)
points(ord1)
```

and let's filter away the outliers and make new plot from above:

```{r}
NMDS_points1 <- as_tibble(ord1$points) %>%
  filter(MDS1 < 50)

title <- "NMDS, MDS1 < 50"
qplot(MDS1, 
      MDS2, 
      data = NMDS_points1, 
      geom = c("point", "density2d")) + ggtitle(title)

path <- "../visualisations/nmds_hmmsearch_avg_MDS1_less_than_50.png"

ggsave(filename = path,
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)
```

Let's filter away slightly more:

```{r}
NMDS_points2 <- as_tibble(ord1$points) %>%
  filter(MDS1 < 1 & MDS1 > -1)

title <- "NMDS, -1 < MDS1 < 1"
qplot(MDS1, 
      MDS2, 
      data = NMDS_points2, 
      geom = c("point", "density2d")) + ggtitle(title)

path <- "../visualisations/nmds_hmmsearch_avg_MDS1_less_than_1_and_greater_than_-1.png"

ggsave(filename = path,
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)
```



# Create lists of fOTUs of interest

## Preview PCA plots

These PCA plots can be used to visually determine potential groupings.

```{r}

PC12s <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC1 = unname(fOTUsHMM_pca$x[,1]), 
              PC2 = unname(fOTUsHMM_pca$x[,2]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC1, 
      PC2, 
      data = PC12s, 
      geom = c("point", "density2d")) + ggtitle(title)

ggsave(filename = "../visualisations/pca_hmmsearch_avg_PC1-2.png",
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)

PC23s <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC2 = unname(fOTUsHMM_pca$x[,2]), 
              PC3 = unname(fOTUsHMM_pca$x[,3]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC2, 
      PC3, 
      data = PC23s, 
      geom = c("point", "density2d")) + ggtitle(title)

ggsave(filename = "../visualisations/pca_hmmsearch_avg_PC2-3.png",
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)

PC34s <- tibble(fOTUs = names(fOTUsHMM_pca$x[,1]), 
              PC3 = unname(fOTUsHMM_pca$x[,3]), 
              PC4 = unname(fOTUsHMM_pca$x[,4]))

title <- "PCA of e-values of matches in fOTUs"
qplot(PC3, 
      PC4, 
      data = PC34s, 
      geom = c("point", "density2d")) + ggtitle(title)

ggsave(filename = "../visualisations/pca_hmmsearch_avg_PC3-4.png",
       device = png, 
       width = 1000, 
       height = 750, 
       units = "in",
       limitsize = FALSE,
       dpi = 320)
```

## Create a file for each list

### Create file names

```{r}
# Path to files
path <- "../analyses/lists_of_PCA_clusters/"
# Names of files
PC12a_filename <- paste(path,"fOTU_PC1-and-2_x--100-to-0-and-y--100-to-+100.txt", sep = "", collapse = NULL)
PC12b_filename <- paste(path,"fOTU_PC1-and-2_x-+250-to-+400-and-y--100-to-0.txt", sep = "", collapse = NULL)
PC23_filename <- paste(path,"fOTU_PC2-and-3_x--100-to-+50-and-y--100-to-+100.txt", sep = "", collapse = NULL)
PC34_filename <- paste(path,"fOTU_PC3-and-4_x--50-to-+50-and-y--50-to-+75.txt", sep = "", collapse = NULL)

```

### Filter tibbles based on coordinates

```{r}
# This function filters a tibble based on given limit values
filterator <- function(data, x_low, x_high, y_low, y_high){
  data %>% 
    filter(data[,2] > x_low & data[,2] < x_high & data[,3] > y_low & data[,3] < y_high) %>%
    select(fOTUs)
}
```


```{r}
PC12_cluster1 <- filterator(PC12s,-100,0,-100,100) 
PC12_cluster2 <- filterator(PC12s,250,400,-100,0) 
PC23_cluster <- filterator(PC23s,-100,50,-100,100)
PC34_cluster <- filterator(PC34s,-50,50,-50,75)
```

### Create the files

```{r}
write_delim(PC12_cluster1, 
            PC12a_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
write_delim(PC12_cluster2, 
            PC12b_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
write_delim(PC23_cluster, 
            PC23_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
write_delim(PC34_cluster, 
            PC34_filename, 
            delim = "", 
            na = "NA", 
            append = FALSE,
            col_names = FALSE, 
            quote_escape = "double")
```


# Not used at the moment

This following is not run because they are not of interest at the moment.

```{r eval=FALSE, include=TRUE}
fOTUbinNums <- read_csv(file = "../analyses/numBinsfOTU.csv", 
                          col_types = "ci", 
                          col_names = c("fOTU_name",
                                        "num_bins"))
egg_nog_cats <- 
  read_tsv(file = "../data/annotations/10239_annotations.tsv", 
           col_types = "fcfc", 
           col_names = c("taxid","hmm_profile_id",
                         "egg_nog_category",
                         "hmm_description")) %>%
  # Drop taxid because it's uninteresting
  select(.,-taxid)

num_seqs <- read_csv(file = "../analyses/numSeqsBin.csv", 
                          col_types = "ci")
```


```{r eval=FALSE, include=TRUE}
# Make a big tibble out of these three tibbles by left joining 
fOTU <- left_join(fOTU, 
                  egg_nog_cats, 
                  by = "hmm_profile_id") %>%
  left_join(., 
            fOTUbinNums, 
            by = "fOTU_name") %>%
  # Remove values that were outside inclusion threshold
  filter(.,inside_inclusion_threshold) %>%
  # Drop inside_inclusion_threshold now that it has done its duty
  select(.,-inside_inclusion_threshold)

  # Find how many hits there are to each bin
  numHits <- fOTU %>%
    count(.,Target_Bin_id) %>%
    rename(.,Num_hits = n)
  
  # Add num hits beside each bin
  fOTU <- left_join(fOTU, numHits, by = "Target_Bin_id")
  
  # Add total number of sequences in each bin
  fOTU <- left_join(fOTU, num_seqs, by = "Target_Bin_id")
  
  # Count how many unique hits to bins there are
  fOTU_vec_len <- fOTU %>%
    distinct(.,Target_Bin_id) %>%
    pull(Target_Bin_id) %>%
    length()
  
  # Grab the total number of bins in the fOTU
  fOTU_tot_num_bins <- fOTU %>%
    pull(num_bins)
  fOTU_tot_num_bins <- fOTU_vec_len/fOTU_tot_num_bins[1]
  
  # Append the value to a vector
#  hit_percentage <- c(hit_percentage,fOTU_tot_num_bins)

#}

# Create finally a tibble from the vector
#hits <- tibble(hit_percentage)
```


# Session info

```{r}
sessionInfo()
```


# References
